---
title: "POLI7002:Homework 4"
author: "Hu, Yue"
date: '`r Sys.Date()`'
output: html_notebook
---

# Data manipulation
Package preload:
```{r}
library(pacman)
p_load(ggplot2, # Visualization
       rjson, quanteda, tm, # analysis
       haven, descr, stringr, data.table, feather, 
       tibble, purrr, dplyr) # data wrangling
```

The `quanteda` is good at Luke dictionary (`liwc`).

Input the dictionary and text files.
In the dictionary, there are multiple categories.
What we are interested in in this project is "positive" and "negative."

```{r}
liwc <- dictionary(file = "LIWC2007.txt",format = "LIWC")
names(liwc)
#load trump download
trump <- fromJSON(file = "../hw3/trump.json")
```

The input `json` is a list of texts and their meta data. 
The following step is thus to draw the texts out and convert it to a `VCorpus`, so as to inputing into `quanteda` functions.

```{r}
trump_articles <- map(trump, "text")
trump_corpus <- Corpus(VectorSource(trump_articles)) %>% corpus()

# count the words
trump_wc <- dfm(trump_corpus) %>% # creating a document-feature matrix
  rowSums()
head(trump_wc)
```

Then we can use the dictionary file to analyze the `document-feature matrix` (viz., the DTM) just got.
NB: the assignement requires custom stopwords. 

```{r}
# customize stopwords
stopCust <- c(stopwords("english"), "america", "trump", "donald", "rubio", "hillary")

trump_dfm <- dfm(trump_corpus,  # corpus
                 dictionary = liwc, # dictionary file
                 removeNumbers = TRUE,  # remove the digits
                 removePunct = TRUE,  # remove the punctuations
                 ignoredFeatures = stopCust) # tokenization
```

What we are looking for are the "positive" and "negative" categories.
So, we search where the root *pos* and *neg* locate according to the index stored in the `trump_dfm@Dimnames$features`.
Here we use the basic `grep` function, since it gives the position directly.
`stringi::str_locale_get` seems having the same function---but, why bothers~

```{r}
grep("pos", trump_dfm@Dimnames$features, value = T)
grep("neg", trump_dfm@Dimnames$features, value = T)
```

The goal here is to create a bar chart in R that shows the percentage of positive and negative words for each month in your data.
To achieve this, we need not only to know the distribution of each article, but also to aggregate the data by months. 
First, let's merge time tags with the sentiment data.

```{r}
trump_date <- map_chr(trump, "date") %>% # capture the data
  unname() %>%
    map_dbl(function(date) as.Date(date, format = "%B %d, %Y")) %>%
      as.POSIXct.Date() # the hms are redundent, but just leave them there.

trumpSent <- data.frame(date = trump_date, pos = as.numeric(trump_dfm[,grep("pos", trump_dfm@Dimnames$features)]), 
                        neg = as.numeric(trump_dfm[,grep("neg", trump_dfm@Dimnames$features)[2]]))

head(trumpSent)
```

Now let's aggregate the data by month using the `dplyr` functions.
To better presented with `ggplot2` functions, we reshape the original "wide" data to a "long" one.

```{r}
trumpMonth <- mutate(trumpSent, month = lubridate::month(date, label = T)) %>%
  # get the months from the dates
  group_by(month) %>%
    summarise(sumPos = sum(pos), sumNeg = sum(neg)) %>% 
  # calculate the monthly counts
      tidyr::gather(sentiment, counts, sumPos:sumNeg)
  # reshape the data

ggplot(data = trumpMonth, aes(x = month, y = counts, fill = sentiment))+
  geom_bar(stat = "identity", position=position_dodge()) +
  # bar graph basic
  xlab("Month") + ylab("Sum of Word Counts") +
  # axes labels
  ggtitle("Dictionary-Based Sentiment Analysis", subtitle = "Trump-related Articles from NYT") + 
  # title
  scale_fill_discrete(name="Sentiment",breaks=c("sumPos", "sumNeg"),
                      labels=c("Positive", "Negative"))
  # legend

```

The next task is to replicate the above analysis but with a custom dictionary. 
To simplify the job, I just subset the "money" and "sexual" items from the LIWC dictionary (see the full list by `names(liwc)`.
Let's see how much the articles about Trump involve these issues.

```{r}
dictSim <- list(money = liwc[["money"]],
                sex = liwc[["sexual"]])

map(dictSim, head)
```

Then let's rerun the analysis.
```{r}
trump_dfm <- dfm(trump_corpus,  # corpus
                 dictionary = dictSim, # dictionary file
                 removeNumbers = TRUE,  # remove the digits
                 removePunct = TRUE,  # remove the punctuations
                 ignoredFeatures = stopCust) # tokenization

trumpSent <- data.frame(date = trump_date, money = as.numeric(trump_dfm[,1]), 
                        sex = as.numeric(trump_dfm[,2]))
# no need to grep, since we know there are only two categories.

trumpMonth <- mutate(trumpSent, month = lubridate::month(date, label = T)) %>%
  # get the months from the dates
  group_by(month) %>%
    summarise(money = sum(money), gender = sum(sex)) %>% 
  # calculate the monthly counts
      tidyr::gather(sentiment, counts, money:gender)
  # reshape the data

ggplot(data = trumpMonth, aes(x = month, y = counts, fill = sentiment)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  # bar graph basic
  xlab("Month") + ylab("Sum of Word Counts") +
  # axes labels
  ggtitle("Dictionary-Based Issue Analysis", subtitle = "Trump-related Articles from NYT") + 
  # title
  scale_fill_discrete(name = "Issue")
  # legend
```

